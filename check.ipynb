{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b8f2901",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-02 15:34:40.806103: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-06-02 15:34:40.809587: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-06-02 15:34:40.818947: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748853280.833814    2644 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748853280.838285    2644 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1748853280.851073    2644 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748853280.851087    2644 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748853280.851089    2644 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748853280.851091    2644 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-02 15:34:40.855357: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22896 files belonging to 2 classes.\n",
      "Found 2514 files belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1748853284.186332    2644 cuda_executor.cc:1228] INTERNAL: CUDA Runtime error: Failed call to cudaGetRuntimeVersion: Error loading CUDA libraries. GPU will not be used.: Error loading CUDA libraries. GPU will not be used.\n",
      "W0000 00:00:1748853284.188843    2644 gpu_device.cc:2341] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6206 files belonging to 2 classes.\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-02 15:34:44.506952: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "DATA_DIR = \"data_pre\"\n",
    "IMG_HEIGHT = 128\n",
    "IMG_WIDTH = 128\n",
    "CHANNELS = 3\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    f\"{DATA_DIR}/train\",\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"binary\",\n",
    "    image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    f\"{DATA_DIR}/val\",\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"binary\",\n",
    "    image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    f\"{DATA_DIR}/test\",\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"binary\",\n",
    "    image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "for images, labels in train_ds.take(1):\n",
    "    print(labels.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4054e3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs available:\n",
      "  PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(\"GPUs available:\")\n",
    "    for gpu in gpus:\n",
    "        print(f\"  {gpu}\")\n",
    "else:\n",
    "    print(\"No GPUs found. TensorFlow will run on CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55fe2e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- JAX Device Information ---\n",
      "JAX detected devices:\n",
      "  TFRT_CPU_0\n",
      "\n",
      "WARNING: No GPU devices found. JAX is likely running on CPU.\n",
      "JAX default backend: cpu\n",
      "\n",
      "JAX XLA bridge platform: cpu\n",
      "\n",
      "JAX version: 0.4.28\n",
      "jaxlib version: 0.4.28\n",
      "CUDA runtime version info not directly available through jax.lib.xla_client.\n",
      "cuDNN version info not directly available through jax.lib.xla_client.\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "\n",
    "# --- PRIMARY CHECK: This is the most reliable way ---\n",
    "print(\"--- JAX Device Information ---\")\n",
    "devices = jax.devices()\n",
    "print(\"JAX detected devices:\")\n",
    "for device in devices:\n",
    "    print(f\"  {device}\")\n",
    "\n",
    "gpu_devices = [d for d in devices if d.device_kind == 'gpu']\n",
    "if gpu_devices:\n",
    "    print(f\"\\nSUCCESS: JAX is configured to use GPU backend. Found {len(gpu_devices)} GPU(s).\")\n",
    "    print(f\"JAX default backend: {jax.default_backend()}\")\n",
    "else:\n",
    "    print(\"\\nWARNING: No GPU devices found. JAX is likely running on CPU.\")\n",
    "    print(f\"JAX default backend: {jax.default_backend()}\")\n",
    "\n",
    "# --- Alternative/Internal Check (if you're curious, but less critical) ---\n",
    "# The previous error indicates 'get_backend()' moved or was removed from xla_extension\n",
    "# The correct way to get the backend through the bridge is:\n",
    "try:\n",
    "    print(f\"\\nJAX XLA bridge platform: {jax.lib.xla_bridge.get_backend().platform}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error accessing jax.lib.xla_bridge.get_backend().platform: {e}\")\n",
    "\n",
    "print(f\"\\nJAX version: {jax.__version__}\")\n",
    "print(f\"jaxlib version: {jax.lib.__version__}\")\n",
    "\n",
    "# Check for CUDA/cuDNN version info again (still might not be available directly)\n",
    "try:\n",
    "    # This path depends on jaxlib's internal structure and can change.\n",
    "    # It might be in jax.lib.xla_client or jax._src.lib.xla_client\n",
    "    # Let's try a common path for newer jaxlib versions.\n",
    "    if hasattr(jax.lib, 'xla_client') and hasattr(jax.lib.xla_client, 'cuda_runtime_version'):\n",
    "        print(f\"Built with CUDA (runtime): {jax.lib.xla_client.cuda_runtime_version()}\")\n",
    "    else:\n",
    "        print(\"CUDA runtime version info not directly available through jax.lib.xla_client.\")\n",
    "\n",
    "    if hasattr(jax.lib, 'xla_client') and hasattr(jax.lib.xla_client, 'cudnn_version'):\n",
    "        print(f\"Built with cuDNN: {jax.lib.xla_client.cudnn_version()}\")\n",
    "    else:\n",
    "        print(\"cuDNN version info not directly available through jax.lib.xla_client.\")\n",
    "\n",
    "except AttributeError:\n",
    "    print(\"CUDA/cuDNN version info not directly available (AttributeError).\")\n",
    "except Exception as e:\n",
    "    print(f\"Error retrieving CUDA/cuDNN info (general): {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qvit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
